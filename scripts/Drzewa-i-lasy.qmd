---
title: "Drzewa decyzyjne i lasy losowe"
author: "Paweł Wieczyński"
format: html
editor: visual
---

```{r}
if(!require('pacman')) install.packages('pacman')
pacman::p_load(tidyverse, caret, rpart, rpart.plot, ipred, randomForest, gbm, xgboost, vip, pdp)
```

## Drzewo klasyfikacyjne

Algorytm tworzenia drzewa decyzyjnego działa następująco:

1.  W pierwszym kroku wybieramy jedną ze zmiennych objaśniających $x_j$ oraz szukamy wartość tej zmiennej (ozn. ją jako próg $t$), która *najlepiej* klasyfikuje zbiór treningowy na klasy $0$ oraz $1$.
2.  Z kroku pierwszego otrzymaliśmy dwa zbiory tzn. obserwacje dla których $x_j > t$ oraz $x_j \leq t$.
3.  W każdym z tych zbiorów powtarzamy procedurę z kroku pierwszego otrzymując kolejne podzbiory. Całość powtarzamy, aż wszystkie podzbiory końcowe mają z góry zadaną ilość obserwacji.

![](images/cart.gif){fig-align="center"}

Jak wybrać *najlepszą* klasyfikację zbioru treningowego? Oznaczmy $p$ proporcję obserwacji z klasy $1$ w danym węźle drzewa decyzyjnego. Wówczas $1-p$ będzie oznaczało proporcję obserwacji z klasy $0$ w tym samym węźle. Możemy zastosować kilka metryk oceniających jak dobrze dany podział drzewa rozdziela klasy:

-   entropia krzyżowa (ang. *cross-entropy* lub *deviance* lub *information*):

    $$
    L(p) =- p \ln p - (1 - p) \ln (1 - p)
    $$

-   współczynnik Giniego:

    $$
    L(p) = 2 p (1- p)
    $$

-   błąd klasyfikacji (ang. *misclassification error*)

    $$
    L(p) = 1 - \max(p, 1 - p)
    $$

```{r}
error_deviance = function(p) -p * log(p) - (1 - p) * log(1 - p)
error_gini = function(p) 2 * p * (1 - p)
error_misclass = function(p) 1 - max(p, 1 - p)


errors_df = tibble(
  p = seq(0, 1, by = 0.001)
) %>%
  rowwise() %>%
  mutate(
    Deviance = error_deviance(p)
    , Gini = error_gini(p)
    , Misclass = error_misclass(p)
  ) %>%
  pivot_longer(
    cols = 2:4
    , names_to = 'Error'
    , values_to = 'Value'
  )

ggplot(errors_df, aes(x = p, y = Value)) +
  geom_point(aes(color = Error), size = 1)
```

### Krok 1: Przygotowanie danych

```{r}
df = read.csv('data\\wine-quality.csv', stringsAsFactors = TRUE)
#map_dbl(df, ~ sum(is.na(.x)))
colSums(is.na(df))
table(df$quality)
```

```{r}
df = read.csv('data\\wine-quality.csv', stringsAsFactors = TRUE)
set.seed(234)
# train_index = createDataPartition(df$quality, p = 0.8, list = FALSE)
train_index = sample(1:nrow(df), size = 0.80 * nrow(df))
df_train = df[train_index, ]
df_test = df[-train_index, ]
```

### Krok 2: Trenowanie modelu

W metodzie `rpart`, która jest jedną z implementacji drzew decyzyjnych, mamy kilka hiperparametrów, które możemy kontrolować:

-   *cp (complexity parameter)* - podział drzewa, tzn. jeśli ten nie zmniejszy błędu dopasowania o zadany procent, to podział nie jest dokonywany

-   *maxdepth* - maksymalna głębokość drzewa (początkowy węzeł liczymy jako 0)

-   *minsplit* - minimalna ilość obserwacji jaka musi istnieć w danym węźle aby doszło do podziału

-   *minbucket* - minimalna ilość obserwacji jaka musić istnieć w węzłach końcowych

-   *xval* - ilość walidacji krzyżowych.

Ponadto w *rpart* możemy wybrać czy podział ma być wykonany na podstawie indeksu Giniego czy na podstawie entropii krzyżowej.

```{r}
model_1 = rpart(
  quality ~ .
  , data = df_train
  # , minsplit = 100
  , minbucket = 10
  # , cp = 0.01
  , xval = 1
  , parms = list(split = 'gini')
)
# parms = list(split = 'information or gini')
# summary(model_1)

rpart.plot(model_1, type = 2)

model_2 = rpart(
  quality ~ .
  , data = df_train
  # , minsplit = 100
  , minbucket = 10
  # , cp = 0.01
  , xval = 1
  , parms = list(split = 'information')
)

rpart.plot(model_2, type = 2)
```

Duże drzewo, tzn. takie z dużą ilością węzłów końcowych może prowadzić do przetrenowania modelu (małe obciążenie na zbiorze treningowym, duża wariancja). Z kolei małe drzewo może nie mieć żadnej mocy prognostycznej. Rozmiar drzewa jest zatem hiperparametrem, który powinnismy zoptymalizować.

W praktyce najpierw budujemy duże drzewo, a potem je przycinamy (ang. *tree-pruning*).

```{r}
model_3 = rpart::prune.rpart(model_2, cp = 0.1)
rpart.plot(model_3, type = 2)
```

### Krok 3: Prognozowanie i ocena modelu

```{r}
y = df_test$quality
length(y)

y_hat = predict(model_2, newdata = df_test) %>%
  .[, 1] %>%
  as.numeric()

y_hat = ifelse(y_hat > 0.5, 'bad', 'good') %>%
  as.factor()
  
levels(y) == levels(y_hat)

confusionMatrix(
  data = y_hat
  , reference = y
)


```

```{r}
y_hat = predict(model_3, newdata = df_test) %>%
  .[, 1] %>%
  as.numeric()

y_hat = ifelse(y_hat > 0.5, 'bad', 'good') %>%
  as.factor()
  
levels(y) == levels(y_hat)

confusionMatrix(
  data = y_hat
  , reference = y
)
```

### Interpretacja zmiennych objaśniających

```{r}
# Istotnośc zmiennych / variable importance plot
vip(model_1)

# partial dependence plot
partial(model_2, 'alcohol') %>% autoplot()
```

### Biblioteka caret

Biblioteka `caret` zapewnia infrastrukturę to optymalizacji hiperparametrów w wielu algorytmach uczenia maszynowego.

W funkcji `trainControl` definiujemy metodę (w tym wypadku będzie to walidacja krzyżowa).

```{r}
control = trainControl(
  method = 'cv'
  , number = 5
)
```

W funkcji `train` oprócz formułki i zbioru danych podajemy następujące argumenty:

-   `method` - jaki algorytm uczenia maszynowego chcemy zastosować. List obsługiwanych algorytmów przez bibliotekę `caret`: <https://topepo.github.io/caret/train-models-by-tag.html>

-   `metric` - jaką miarę dopasowania chcemy zoptymalizować

-   `tuneLength` lub `tuneGrid` - siatka hiperparametrów do optymalizacji

-   `trControl` - metoda optymalizacji hiperparametrów

```{r}
set.seed(213)
model_3 = caret::train(
  quality ~ . # formułka co jest zmienną celu, co jest zmiennymi objaśniającymi
  , data = df_train # zbiór danych 
  , method = 'rpart' # jaki algorytm
  
  # 1 sposób - ilość parametrów do sprawdzenia zostanie automatycznie dobrana
  , tuneLength = 20
  
  # 2 sposób - ręcznie podajemy jakie kombinacje parametrów chcemy optymzalić
  # , tuneGrid = expand.grid(cp = seq(0.01, 1, by = 0.01))
  
  # Metoda walidacji krzyżowej
  , trControl = trainControl(method = 'cv', number = 5)
  
  # Opcjonalnie dodatkowe parametry związane z wybranym algorytmem
  # , control = rpart.control(maxdepth = 2)
)

model_3

ggplot(model_3)

```

```{r}
# Prognoza na zbiorze testowym
y_hat = predict(model_3, newdata = df_test)

confusionMatrix(
  data = y_hat
  , reference = y
)
```

### Zadania

1.  Wczytaj zbiór danych `baseball.csv` (narazie bez podziału na zbiór treningowy i testowy). Dopasuj model regresji liniowej oraz drzewo regresyjne. Który model lepiej dopasowuje się do danych?

    ```{r}

    ```

2.  Wykonaj wykres 3D, aby zobaczyć jak wyglądają modele z zadania 1. Można użyć biblioteki `plotly`.

    ```{r}

    ```

3.  Wczytaj zbiór danych `kc_house_data.csv`. Usuń niepotrzebne zmienne.

    ```{r}

    ```

4.  Podziel dane na zbiór treningowy oraz testowy (jeśli możliwe to wykorzystaj ten sam podział jaki był wykorzystany w zadaniu 2 z regresji liniowej, co pozwoli porównać wyniki :) )

    ```{r}

    ```

5.  Znajdź optymalne parametry drzewa decyzyjnego, aby zminimalizować błąd prognozy na zbiorze walidacyjnym (wskazówka: za pomocą funkcji `caret::train` możemy kontrolowac `cp` lub `maxdepth`.

    ```{r}

    ```

6.  Dokonaj finalnej oceny modelu na zbiorze testowym.

    ```{r}


    ```

Dla porównania model regresji liniowej

```{r}
model_house_reg = lm(price ~., data = df_train)
y_hat_reg = predict(model_house_reg, df_test)
RMSE(y_hat_reg, y)
```

## Bagging

Drzewa losowe cechują się dużą wariancją, tzn. niewielka zmiana w danych lub niewielka zmiana parametrów modelu może sprawić, że struktura drzewa będzie wyglądać zupełnie inaczej. Aby ustabilizować wariancję, możemy zastosować procedurę *bootstrap aggregating*:

1.  Losujemy $B$ próbek ze zwracaniem tego samego rozmiaru co dane wejściowe.

2.  Na każdej próbce budujemy drzewo decyzyjne.

3.  Wykonujemy prognozy na zbiorze testowym dla każdego modelu.

4.  Uśredniamy prognozy.

```{r}
df = read.csv('data\\wine-quality.csv', stringsAsFactors = TRUE)
set.seed(234)
train_index = sample(1:nrow(df), size = 0.80 * nrow(df))
df_train = df[train_index, ]
df_test = df[-train_index, ]

model_4 = ipred::bagging( 
  quality ~ . # formułka co modelujemy
  , data = df_train # zbiór danych
  , nbagg = 100 # ilość drzew decyzyjnych
  , coob = TRUE
  
  # opcjonalnie dodatkowe parametry związane z modelem
  # , minsplit = 100
  # , minbucket = 10
  # , cp = 0.01
  # , xval = 1
  # , parms = list(split = 'gini')
)

model_4
```

Błąd *out-of-bag* możemy potraktować jako wbudowany mechanizm oszacowania błędu na zbiorze walidacyjnym.

```{r}
y = df_test$quality
y_hat = predict(model_4, newdata = df_test, type = 'prob') %>%
  .[, 1]
  as.numeric()

y_hat = ifelse(y_hat > 0.5, 'bad', 'good') %>%
  as.factor()
  
levels(y) == levels(y_hat)

confusionMatrix(
  data = y_hat
  , reference = y
)
```

Przyjrzyjmy się kilku przykładowym drzewom decyzyjnym w procedurze *baggingu*.

```{r}
# Ręczna procedura baggingu
set.seed(213)
B = 5 # ilośc drzewek
n = nrow(df_train) # rozmiar danych / ilośc obserwacji

for (b in 1:B) {
  # Losujemy obserwacje ze zwracaniem
  boot_indices = sample(
    1:n
    , size = n
    , replace = TRUE
  )
  
  # wybieramy wylosowany zbiór danych
  df_boot = df_train[boot_indices, ]
  
  # dopasowujemy model na wylosowanej próbkce
  boot_model = rpart(
    quality ~ .
    , data = df_boot
    , minsplit = 100
    , maxdepth = 3
    , cp = 0.01
    , xval = 1
    , parms = list(split = 'gini')
  )
  
  # wykres 
  rpart.plot(boot_model, type = 2)
}
```

Wszystkie drzewa mają podobną strukturę, tzn. te same zmienne są używane do podziału w każdym kroku. Mówimy, że drzewa są *skorelowane*.

### Lasy losowe

Szczególnym przypadkiem *baggingu* są lasy losowe. Aby uzyskać jeszcze większą losowość, czyli zdekorelować drzewa decyzyjne, możemy w każdym kroku wybierać zmienne objaśniające w sposób losowy.

```{r}
set.seed(213)
B = 5
n = nrow(df_train)
k = ncol(df_train) # ilość wszystkich kolumn

for (b in 1:B) {
  # Losujemy obserwacje ze zwracaniem
  boot_indices = sample(
    1:n
    , size = n
    , replace = TRUE
  )
  
  # Losujemy kolumny
  boot_columns = sample(
    1:(k-1)
    , size = 5
    , replace = FALSE
  )
  
  # wybieramy wylosowany zbiór
  df_boot = df_train[boot_indices, c(boot_columns, k)]
  
  boot_model = rpart(
    quality ~ .
    , data = df_boot
    , minsplit = 100
    , maxdepth = 3
    , cp = 0.01
    , xval = 1
    , parms = list(split = 'gini')
  )
  
  rpart.plot(boot_model, type = 2)
}
```

W bibliotece `randomForest` mamy poprawną oraz efektywną implementację tej procedury. Główne hiperparametry w lasach losowych to:

-   `mtry` - ilość zmiennych objaśniających wybieranych w sposób losowy przy każdym podziale drzewa

-   `ntree` - ilość drzew w lesie :)

```{r}
model_5 = randomForest(
  quality ~.
  , data = df_train
  , mtry = 2
  , ntree = 1000
)

model_5
```

```{r}
y = df_test$quality
y_hat = predict(model_5, newdata = df_test, type = 'prob') %>%
  .[, 1]
  as.numeric()

y_hat = ifelse(y_hat > 0.5, 'bad', 'good') %>%
  as.factor()
  
levels(y) == levels(y_hat)

confusionMatrix(
  data = y_hat
  , reference = y
)
```

```{r}
model_rf_caret = caret::train(
  quality ~ .
  , data = df_train
  , method = 'rf'
  , tuneLength = 5
  , trControl = trainControl(method = 'cv', number = 3)
  , control = list(ntree = 100)
)

model_rf_caret
```

### Zadania

```{r}
house_data = read.csv('data\\kc_house_data.csv') %>%
  select(-id, -date)
set.seed(213)
# Wyodrębnienie zbioru testowego
train_index = sample(
  1:nrow(house_data)
  , size = floor(0.75 * nrow(house_data))
)
 
df_train = house_data[train_index, ]
df_test = house_data[-train_index, ]
```

1.  Do danych `kc_house_data` dopasuj model *baggingu*. Przy ilu drzewach wartość błędu testowego się ustabilizuje?

2.  Do danych `kc_house_data` dopasuj las losowy oraz znajdź optymalną wartość parametru `mtry`. Można skorzystać z biblioteki `caret`.

3.  Jak wygląda las losowy w 3D dla danych `baseball.csv`?

## Boosting

*Bagging* jest algorytmem równoległym, tzn. trenujemy wiele modeli niezaleznie, a następnie uśredniamy wyniki. W przeciwieństwie do *baggingu*, mamy dużo algorytmów sekwencyjnych, znanych pod nazwą *boosting*. Tu równiez budujemy pewną ilość podobnych modeli, ale każdy kolejny model jest w pewien sposób zależny od poprzedniego. Najpopularniejsze algorytmy to **AdaBoost** (od *Adaptive Boosting*), **GBM** (od *Gradient Bossted Machines*) oraz **XGBoost** (od *eXtreme Gradient Boosting*).

### XGBoost

Na temat szczegółów algorytmu *XGBoost* możemy poczytać w oficjalnej dokumentacji: <https://xgboost.readthedocs.io/en/stable/tutorials/model.html>

Obecnie jest to jeden z najlepszych algorytmów jeśli chodzi o minimalizację funkcji straty. W praktyce bardzo często zaczynamy od oszacowania błędu generalizacji z algorytmu *XGBoost*, a następnie stosujemy go jako benchmark jeśli chcemy znaleźć lepsze oszacowanie danego problemu. Ponadto implementacja tego algorytmu jest zoptymalizowana pod kątem przetwarzania wielowątkowego/równoległego, zarządzania pamięcią operacyjną itp, co czyni obliczenia na dużych zbiorach danych efektywnymi.

Model z *"ustawieniami fabrycznymi":*

```{r}
df = read.csv('data\\kc_house_data.csv') %>%
  dplyr::select(-id, -date)

set.seed(213)
train_indices = sample(
  1:nrow(df)
  , size = 0.70 * nrow(df)
  , replace = FALSE
)

df_train = df[train_indices, ]
df_test = df[-train_indices, ]

# Dane muszą być w specjalnym formacie danych dedykowanym algorytmowi XGBoost
df_train_xgb = df_train %>%
  select(-price) %>%
  as.matrix() %>%
  xgb.DMatrix(label = df_train$price)

df_test_xgb = df_test %>%
  select(-price) %>%
  as.matrix() %>%
  xgb.DMatrix(label = df_test$price)

m = 100 # ilosc drzew

model_xgb = xgboost(
  data = df_train_xgb
  , params = list(booster = 'gbtree')
  , nrounds = m
  , verbose = 1
)

# Prognoza na zbiorze treningowym i testowym dla różnych ilosci drzew
rmse_xgb_df = tibble(ntrees = 1:m) %>%
  
  mutate(train = map_dbl(1:m, ~ predict(
    model_xgb
    , newdata = df_train_xgb
    , iterationrange = c(1,.x)
  ) %>% RMSE(obs = df_train$price))) %>%
  
  mutate(test = map_dbl(1:m, ~ predict(
    model_xgb
    , newdata = df_test_xgb
    , iterationrange = c(1,.x)
  ) %>% RMSE(obs = df_test$price))) %>%
  
  pivot_longer(
    cols = 2:3
    , names_to = 'error'
    , values_to = 'value'
  )

ggplot(rmse_xgb_df, aes(x = ntrees, y = value)) +
  geom_line(aes(color = error), size = 1)

# błąd RMSE
min(rmse_xgb_df %>% filter(error == 'test') %>% pull(value))

vip(model_xgb)
```

Oprócz *"ustawień fabrycznych"* mamy możliwość optymalizacji kilku hiperparametrów w tym modelu:

-   $\eta$ (*eta*) - współczynnik szybkości uczenia się modelu (ang. *learning rate*)

-   $\gamma$ (*gamma*) - minimalna wartość o jaką musi zmniejszyć się funkcja straty, aby dokonać podział w pojedynczym drzewie decyzyjnym (używane podczas przycinania drzewa)

-   *max_depth* - maksymalna głębokość drzew decyzyjnych

-   *subsample* - ile procent obserwacji zbioru treningowego ma być wylosowane do budowy pojedynczego drzewa decyzyjnego

-   *colsample_bytree* - ile procent atrybutów/kolumn ma być wylosowane do budowy pojedynczego drzewa decyzyjnego

-   $\alpha$ (*alpha*) - regularyzacja, jeśli podejrzewamy że część atrybutów może być nieistotna (analogicznie jak w regresji lasso)

-   $\lambda$ (*lambda*) - regularyzacja, jeśli podejrzewamy że część atrybutów może być wzajemnie skorelowana (analogicznie jak w regresji grzbietowej).

Strategia optymalizacji hiperparametrów może wyglądać następująco:

1.  Wybieramy $0.05 \leq \eta \leq 0.3$.
2.  Metodą walidacji krzyżowej szacujemy optymalną ilość drzew.
3.  Metodą walidacji krzyżowej szukamy optymalne parametry służące do budowy drzew decyzyjnych: *max_depth*, *gamma*, *subsample*, *colsample_bytree*.
4.  Jeśli model jest przeszacowany, to szukamy optymalne parametry regularyzacyjne: $\alpha$ oraz $\lambda$. Na przeszacowanie może wskazywać duża różnica między błędem treningowym a błędem walidacji krzyżowej.
5.  Ewentualne obniżenie $\eta$.

```{r}
# Pierwszy krok - ustalamy eta = 0.3 
# i szukamy optymalną ilośc drzew metodą CV
xgb_paramas = list(
  booster = 'gbtree'
  , objective = 'reg:squarederror'
  , eta = 0.3
  , nthread = 11
)

model_1 = xgb.cv(
  params = xgb_paramas
  , data = df_train_xgb
  , nrounds = 500 # maksymalna ilośc drzewek
  , nfold = 5 # ilość foldów w walidacji krzyżowej
  , early_stopping_rounds = 50 # wczesne zatrzymanie, tzn. jesli po 50 iteracjach błąd walidacji się nie poprawi do zatrzymujemy algorytm
)

model_1
m = model_1$best_ntreelimit

```

```{r}
# Krok drugi - optymalizacja hiperparametrów metodą CV
params_grid = expand.grid(
  maxdepth = seq(3, 9, by = 2)
  , gamma = seq(0, 0.5, by = 0.1)
  # , subsample = seq()
  # , colsample_by_tree = seq()
) %>%
  mutate(Validation_Error = NA)

# pętal idzie około minutę
for (i in 1:nrow(params_grid)) {
  
  model_2 = xgb.cv(
    params = xgb_paramas
    , data = df_train_xgb
    , nrounds = m # optymalna ilość drzew z poprzedniego kroku
    , nfold = 5
    , early_stopping_rounds = 50
    # Parametry do optymalizacji
    , maxdepth = params_grid$maxdepth[i]
    , gamma = params_grid$gamma[i]
    
    # Wyłącznie informacji na konsoli
    , verbose = 0
  )
  
  params_grid$Validation_Error[i] = min(model_2$evaluation_log$test_rmse_mean)
  
  cat(i, '\n')
}

ggplot(params_grid, aes(x = gamma, y = Validation_Error)) +
  geom_line(aes(color = as.factor(maxdepth)), size = 1)
```

```{r}
# Krok dwa i pół - zawęzić obszar poszukiwań
# Krok trzeci - jeśli jest przetrenowanie, to zoptymalizować alpha i lambda
# Krok czwarty - obniżenie learning eta
model_final = xgboost(
  params = list(
    booster = 'gbtree'
    , maxdepth = 7
    , gamma = 0.4
    , eta = 0.1 # zmniejszamy learning rate, wcześniej było 0.3
  )
  , data = df_train_xgb
  , nrounds = m*5 # przy zmniejszeniu learning rate warto zwiększyć ilość drzewek
)

y_hat = predict(model_final, df_test_xgb)
RMSE(pred = y_hat, obs = df_test$price)
```
